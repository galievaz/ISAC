Information retrieval
Download and extract the abstracts from [UCI] AAAI-14 Accepted Papers - Papers.csv file located at https://archive.ics.uci.edu/ml/datasets/AAAI+2014+Accepted+Papers. The abstracts will be used as text documents for information retrieval tasks. (Alternatively, download the already extracted data from UIS.)
Implement a keyword-based search (you might use, e.g., queries "active learning", "machine learning text") that will find 10 top ranked documents from the collection. Each word from a query is treated independently. It means, that using a query "active learning", documents containing words "active" and "learning" become relevant. The words do not need to appear in the same order as in the query and there might be other words between them. You do not have (but you can) to use an inverted index.
Your system will rank the documents according to the similarity to a query using the following measures:
    • simple shared words count measure,
    • shared words count with bonus measure,
    • cosine similarity measure using only the term frequency weight,
    • cosine similarity measure using the term frequency * inverse document frequency weight.
Compare and discuss the results.

In some situations, the whole document can be used as a query. Try to use this abstract as a query:
A common bottleneck in deploying supervised learning systems is collecting human-annotated examples. In many domains, annotators form an opinion about the label of an example incrementally --- e.g., each additional word read from a document or each additional minute spent inspecting a video helps inform the annotation. In this paper, we investigate whether we can train learning systems more efficiently by requesting an annotation before inspection is fully complete --- e.g., after reading only 25 words of a document. While doing so may reduce the overall annotation time, it also introduces the risk that the annotator might not be able to provide a label if interrupted too early. We propose an anytime active learning approach that optimizes the annotation time and response rate simultaneously. We conduct user studies on subsets of two document classification datasets and develop simulated annotators that mimic the users. Our simulated experiments show that anytime active learning outperforms several baselines on these two datasets. For example, with an annotation budget of one hour, training a classifier by annotating the first 25 words of each document reduces classification error by 17% over annotating the first 100 words of each document. 

Add stop words filtering before searching according to the similarity to the submitted abstract and see what happened.
